{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = pd.read_csv(\"matchapps.csv\")\n",
    "super_df = pd.read_csv(\"superapps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I've noticed:\n",
    "- Robots scoring on reef while not being marked for leave\n",
    "- Crazy amounts scored in some matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robots scoring on reef while not being marked for leave\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robots scoring crazy amounts, imposing reasonable defaults\n",
    "match_df[\n",
    "    (match_df[\"autoCoral.L1\"] > 5) |\n",
    "    (match_df[\"autoCoral.L2\"] > 5) |\n",
    "    (match_df[\"autoCoral.L3\"] > 5) |\n",
    "    (match_df[\"autoCoral.L4\"] > 5) |\n",
    "    (match_df[\"teleCoral.L1\"] > 10) |\n",
    "    (match_df[\"teleCoral.L2\"] > 10) |\n",
    "    (match_df[\"teleCoral.L3\"] > 10) |\n",
    "    (match_df[\"teleCoral.L4\"] > 10)\n",
    "][\n",
    "    [\"metadata.robotTeam\", \"metadata.matchNumber\"] +\n",
    "    [\"autoCoral.L{}\".format(x) for x in range(1, 5)] +\n",
    "    [\"teleCoral.L{}\".format(x) for x in range(1, 5)]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index 335 - update values to:\n",
    "# 4 L1\n",
    "# 8 L3\n",
    "\n",
    "match_df.loc[335, \"teleCoral.L1\"] = 4\n",
    "match_df.loc[335, \"teleCoral.L3\"] = 8\n",
    "\n",
    "match_df[match_df.index == 335][\n",
    "    [\"metadata.robotTeam\", \"metadata.matchNumber\"] +\n",
    "    [\"autoCoral.L{}\".format(x) for x in range(1, 5)] +\n",
    "    [\"teleCoral.L{}\".format(x) for x in range(1, 5)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding New Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random sample match\n",
    "DEBUG and match_df[(match_df[\"metadata.robotTeam\"] == 4201) & (match_df[\"metadata.matchNumber\"] == 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for auto, tele, and endgame score\n",
    "match_df[\"autoScore\"] = (\n",
    "    match_df[\"leftStartingZone\"] * 3 +\n",
    "    match_df[\"autoCoral.L1\"] * 3 +\n",
    "    match_df[\"autoCoral.L2\"] * 4 +\n",
    "    match_df[\"autoCoral.L3\"] * 6 +\n",
    "    match_df[\"autoCoral.L4\"] * 7 +\n",
    "    match_df[\"autoAlgae.processor\"] * 6 +\n",
    "    match_df[\"autoAlgae.netRobot\"] * 4\n",
    ")\n",
    "\n",
    "match_df[\"teleScore\"] = (\n",
    "    match_df[\"teleCoral.L1\"] * 2 +\n",
    "    match_df[\"teleCoral.L2\"] * 3 +\n",
    "    match_df[\"teleCoral.L3\"] * 4 +\n",
    "    match_df[\"teleCoral.L4\"] * 5 +\n",
    "    match_df[\"teleAlgae.processor\"] * 6 +\n",
    "    match_df[\"teleAlgae.netRobot\"] * 4\n",
    ")\n",
    "\n",
    "match_df[\"climb.park\"] = match_df[\"climb\"] == \"park\"\n",
    "match_df[\"climb.shallow\"] = match_df[\"climb\"] == \"shallow\"\n",
    "match_df[\"climb.deep\"] = match_df[\"climb\"] == \"deep\"\n",
    "\n",
    "match_df[\"endgameScore\"] = (\n",
    "    match_df[\"climb.park\"] * 2 +\n",
    "    match_df[\"climb.shallow\"] * 6 +\n",
    "    match_df[\"climb.deep\"] * 12\n",
    ")\n",
    "\n",
    "match_df[\"totalScore\"] = (\n",
    "    match_df[\"autoScore\"] +\n",
    "    match_df[\"teleScore\"] +\n",
    "    match_df[\"endgameScore\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random sample match to verify\n",
    "DEBUG and match_df[match_df[\"metadata.robotTeam\"] == 3255][[\"autoCoral.L1\", \"autoCoral.L2\", \"autoCoral.L3\", \"autoCoral.L4\", \"autoAlgae.processor\", \"autoAlgae.netRobot\", \"autoScore\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random sample match to verify\n",
    "DEBUG and match_df[match_df[\"metadata.robotTeam\"] == 3255][[\"leftStartingZone\", \"teleCoral.L1\", \"teleCoral.L2\", \"teleCoral.L3\", \"teleCoral.L4\", \"teleAlgae.processor\", \"teleAlgae.netRobot\", \"teleScore\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random sample match to verify\n",
    "DEBUG and match_df[match_df[\"metadata.robotTeam\"] == 3255][[\"climb.park\", \"climb.shallow\", \"climb.deep\", \"endgameScore\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random sample match to verify\n",
    "DEBUG and match_df[match_df[\"metadata.robotTeam\"] == 3255][[\"autoScore\", \"teleScore\", \"endgameScore\", \"totalScore\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for whether teams have scored on the side of the reef in auto\n",
    "match_df[\"metadata.robotColorIsBlue\"] = (\n",
    "    (match_df[\"metadata.robotPosition\"] == \"blue_1\") |\n",
    "    (match_df[\"metadata.robotPosition\"] == \"blue_2\") |\n",
    "    (match_df[\"metadata.robotPosition\"] == \"blue_3\")\n",
    ")\n",
    "\n",
    "# Calculate if a team scored on the right side of the reef, from the perspective of the barge\n",
    "match_df[\"autoScoreRightReef\"] = (\n",
    "    match_df[\"metadata.robotColorIsBlue\"] & (match_df[\"placement.deposit1\"] | match_df[\"placement.deposit6\"]) |\n",
    "    ~match_df[\"metadata.robotColorIsBlue\"] & (match_df[\"placement.deposit3\"] | match_df[\"placement.deposit4\"])\n",
    ")\n",
    "\n",
    "# Calculate if a team scored on the left side of the reef, from the perspective of the barge\n",
    "match_df[\"autoScoreLeftReef\"] = (\n",
    "    match_df[\"metadata.robotColorIsBlue\"] & (match_df[\"placement.deposit3\"] | match_df[\"placement.deposit4\"]) |\n",
    "    ~match_df[\"metadata.robotColorIsBlue\"] & (match_df[\"placement.deposit1\"] | match_df[\"placement.deposit6\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a random sample match to verify\n",
    "DEBUG and match_df[match_df[\"autoScoreRightReef\"] | match_df[\"autoScoreLeftReef\"]][[\"metadata.matchNumber\", \"metadata.robotTeam\", \"metadata.robotPosition\", \"placement.deposit1\", \"placement.deposit2\", \"placement.deposit3\", \"placement.deposit4\", \"placement.deposit5\", \"placement.deposit6\", \"autoScoreRightReef\", \"autoScoreLeftReef\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the columns below, find only the average\n",
    "AVG_COLUMNS = (\n",
    "    [\"startingZone.start{}\".format(x) for x in range(1, 4)] +\n",
    "    [\"pickupLocation.source{}\".format(x) for x in range(1, 3)] +\n",
    "    [\"pickupLocation.ground{}\".format(x) for x in range(1, 4)] +\n",
    "    [\"placement.deposit{}\".format(x) for x in range(1, 7)] +\n",
    "    [\"climb.park\", \"climb.shallow\", \"climb.deep\", \"autoScoreRightReef\", \"autoScoreLeftReef\"]\n",
    ")\n",
    "\n",
    "for c in AVG_COLUMNS:\n",
    "    team_df[f\"{c}.avg\"] = match_df.groupby(\"metadata.robotTeam\")[c].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the columns below, find statistical metrics for each (min, max, avg, std)\n",
    "STAT_COLUMNS = (\n",
    "    [\"autoCoral.L{}\".format(x) for x in range(1, 5)] +\n",
    "    [\"teleCoral.L{}\".format(x) for x in range(1, 5)] +\n",
    "    [\"autoAlgae.netRobot\", \"autoAlgae.processor\", \"autoAlgae.remove\"] +\n",
    "    [\"teleAlgae.netRobot\", \"teleAlgae.processor\", \"teleAlgae.remove\"] +\n",
    "    [\"autoScore\", \"teleScore\", \"endgameScore\", \"totalScore\"]\n",
    ")\n",
    "\n",
    "for c in STAT_COLUMNS:\n",
    "    team_df[f\"{c}.min\"] = match_df.groupby(\"metadata.robotTeam\")[c].min()\n",
    "    team_df[f\"{c}.max\"] = match_df.groupby(\"metadata.robotTeam\")[c].max()\n",
    "    team_df[f\"{c}.avg\"] = match_df.groupby(\"metadata.robotTeam\")[c].mean()\n",
    "    team_df[f\"{c}.std\"] = match_df.groupby(\"metadata.robotTeam\")[c].std()\n",
    "\n",
    "    # We also want to calculate these metrics for different variations of matches being dropped.\n",
    "    for n in range(1, 3):\n",
    "        # Start by removing the top and bottom `n` matches for a team by finding\n",
    "        # the indexes of the matches we want to keep\n",
    "        indexes_to_keep = match_df.groupby(\"metadata.robotTeam\").apply(lambda x: x.sort_values(c).iloc[n:-n]).index.get_level_values(1)\n",
    "        filtered_df = match_df[match_df.index.isin(indexes_to_keep)]\n",
    "\n",
    "        # With our new filtered dataframe, compute statistic metrics\n",
    "        team_df[f\"{c}.min.drop{n}\"] = filtered_df.groupby(\"metadata.robotTeam\")[c].min()\n",
    "        team_df[f\"{c}.max.drop{n}\"] = filtered_df.groupby(\"metadata.robotTeam\")[c].max()\n",
    "        team_df[f\"{c}.avg.drop{n}\"] = filtered_df.groupby(\"metadata.robotTeam\")[c].mean()\n",
    "        team_df[f\"{c}.std.drop{n}\"] = filtered_df.groupby(\"metadata.robotTeam\")[c].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG and team_df[team_df.index == 4201][[\"autoCoral.L4.avg\", \"autoCoral.L4.avg.drop1\", \"autoCoral.L4.avg.drop2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG and match_df[match_df[\"metadata.robotTeam\"] == 4201][[\"metadata.robotTeam\", \"autoCoral.L4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df[\"fouls.avg\"] = super_df[[\n",
    "    \"fouls.insideRobot\",\n",
    "    \"fouls.protectedZone\",\n",
    "    \"fouls.pinning\",\n",
    "    \"fouls.multiplePieces\",\n",
    "    \"fouls.cageFoul\",\n",
    "    \"fouls.other\"\n",
    "]].sum(axis=1).groupby(super_df[\"metadata.robotTeam\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG and team_df[\"fouls.avg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df[\"break.avg\"] = super_df[[\n",
    "    \"break.mechanismDmg\",\n",
    "    \"break.batteryFall\",\n",
    "    \"break.commsFail\",\n",
    "    \"break.bumperFall\"\n",
    "]].sum(axis=1).groupby(super_df[\"metadata.robotTeam\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG and team_df[\"break.avg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All copied from last year, didn't actually check if this works\n",
    "def safeDivide(a, b):\n",
    "    return 0 if b == 0 else a / b\n",
    "\n",
    "team_list = match_df[\"metadata.robotTeam\"].unique().tolist()\n",
    "team_list = [team for team in team_list if not np.isnan(team)]\n",
    "\n",
    "team_df[\"Percent of Matches with No Defense\"] = None\n",
    "team_df[\"Percent of Matches with Some Defense\"] = None\n",
    "team_df[\"Percent of Matches with Full Defense\"] = None\n",
    "team_df[\"Main Defense Type\"] = None\n",
    "\n",
    "for team in team_list:\n",
    "    \n",
    "    defense_type_list = super_df.loc[super_df[\"metadata.robotTeam\"] == team, \"defense\"].tolist()\n",
    "    \n",
    "    no_defense_count = 0\n",
    "    some_defense_count = 0\n",
    "    full_defense_count = 0\n",
    "    \n",
    "    for defense_type in defense_type_list:\n",
    "        if defense_type == \"noDef\":\n",
    "            no_defense_count += 1\n",
    "        elif defense_type == \"someDef\":\n",
    "            some_defense_count += 1\n",
    "        elif defense_type == \"fullDef\":\n",
    "            full_defense_count += 1\n",
    "\n",
    "    defense_total_count = sum([no_defense_count, some_defense_count, full_defense_count])\n",
    "        \n",
    "    team_df.at[team, \"Percent of Matches with No Defense\"] = safeDivide(no_defense_count, defense_total_count)\n",
    "    team_df.at[team, \"Percent of Matches with Some Defense\"] = safeDivide(some_defense_count, defense_total_count)\n",
    "    team_df.at[team, \"Percent of Matches with Full Defense\"] = safeDivide(full_defense_count, defense_total_count)\n",
    "    \n",
    "    biggest_defense_count = max([no_defense_count, some_defense_count, full_defense_count])\n",
    "    main_defense_type = \"\"\n",
    "\n",
    "    if biggest_defense_count == no_defense_count:\n",
    "        main_defense_type += \"No Defense \"\n",
    "    if biggest_defense_count == some_defense_count:\n",
    "        main_defense_type += \"Some Defense \"\n",
    "    if biggest_defense_count == full_defense_count:\n",
    "        main_defense_type += \"Full Defense\"\n",
    "    \n",
    "    team_df.at[team, \"Main Defense Type\"] = main_defense_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG and team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df.to_csv(\"caph_2025_sat.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
